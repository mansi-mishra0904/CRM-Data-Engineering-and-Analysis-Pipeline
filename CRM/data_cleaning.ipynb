{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, sum, lit, regexp_extract, initcap, when, mean, date_format, avg, concat,max as spark_max, current_date\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preprocessing: Address any inconsistencies or errors found during validation and clean the data as necessary.\n",
    "\n",
    "## Data Cleaning Process\n",
    "\n",
    "This outlines the steps involved in cleaning and preparing the customer data for further analysis. The process includes handling missing values, detecting and removing duplicates, formatting phone numbers, and ensuring proper capitalization in specified columns.\n",
    "\n",
    "## Steps Overview\n",
    "\n",
    "### Step 1: Count Missing Values Before Filling\n",
    "\n",
    "Count the missing (null) values in each column before applying any filling operations to understand the extent of missing data.\n",
    "\n",
    "### Step 2: Count Duplicates Before Dropping\n",
    "\n",
    "Check for duplicate records in each column to identify potential data integrity issues before removing duplicates.\n",
    "\n",
    "### Step 3: Format and Clean Phone Numbers\n",
    "\n",
    "Process phone numbers to remove any formatting issues and append the correct country code using the provided country codes file.\n",
    "\n",
    "### Step 4: Drop Duplicates Based on Primary Key\n",
    "\n",
    "Remove duplicate records based on the Primary Key` column to ensure each customer is uniquely represented.\n",
    "\n",
    "### Step 5: Fill Missing Values in 'Email' and 'Phone' Columns\n",
    "\n",
    "Fill missing values in the `Email` and `Phone` columns with a default value of `'unknown'` to ensure completeness.\n",
    "\n",
    "### Step 6: Capitalize the First Letter in 'Name' and 'Country' Columns\n",
    "\n",
    "Capitalize the first letter of each word in the `Name` and `Country` columns for consistent formatting.\n",
    "\n",
    "### Step 7: Cross-Validation - Count Missing Values After Filling\n",
    "\n",
    "Recount the missing values in each column after the filling operation to validate that missing data has been appropriately handled.\n",
    "\n",
    "### Step 8: Count Duplicates After Dropping\n",
    "\n",
    "Recheck for duplicates in each column after the removal process to confirm all duplicates have been successfully dropped.\n",
    "\n",
    "### Step 9: Export the Cleaned Data to a CSV File\n",
    "\n",
    "Finally, export the cleaned DataFrame to a CSV file for further use.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reusable Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utility/common_utility.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "class CustomFormatter(logging.Formatter):\n",
    "    # ANSI escape codes for colors\n",
    "    GREEN = \"\\033[92m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "    def format(self, record):\n",
    "        # Add green color to INFO level messages\n",
    "        if record.levelno == logging.INFO:\n",
    "            record.msg = f\"{self.GREEN}{record.msg}{self.RESET}\"\n",
    "        return super().format(record)\n",
    "\n",
    "# Ensure the logs directory exists\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "# Initialize the logger\n",
    "logger = logging.getLogger('DataCleaningLogger')\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove all existing handlers\n",
    "for handler in logger.handlers[:]:\n",
    "    logger.removeHandler(handler)\n",
    "\n",
    "# Create a stream handler\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(CustomFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "# Create a file handler\n",
    "fh = logging.FileHandler('logs/data_cleaning.log')\n",
    "fh.setLevel(logging.INFO)\n",
    "fh.setFormatter(CustomFormatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "\n",
    "# Add handlers to the logger\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:53,784 - DataCleaningLogger - INFO - \u001b[92mStep 1: Loading data from CSV files into DataFrames...\u001b[0m\n",
      "2024-09-02 16:49:54,147 - DataCleaningLogger - INFO - \u001b[92mStep 2: Displaying initial records for validation...\u001b[0m\n",
      "2024-09-02 16:49:54,149 - DataCleaningLogger - INFO - \u001b[92mCustomers DataFrame:\u001b[0m\n",
      "2024-09-02 16:49:54,174 - DataCleaningLogger - INFO - \u001b[92mProducts DataFrame:\u001b[0m\n",
      "2024-09-02 16:49:54,209 - DataCleaningLogger - INFO - \u001b[92mTransactions DataFrame:\u001b[0m\n",
      "2024-09-02 16:49:54,243 - DataCleaningLogger - INFO - \u001b[92mInteractions DataFrame:\u001b[0m\n",
      "2024-09-02 16:49:54,282 - DataCleaningLogger - INFO - \u001b[92mSales_team DataFrame:\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------------+----------------------+-------------+----------------+\n",
      "|Customer_ID                         |Name             |Email                 |Phone        |Country         |\n",
      "+------------------------------------+-----------------+----------------------+-------------+----------------+\n",
      "|a85e6a90-78d5-490c-a53f-c58b2e57c59b|Shannon Deleon   |NULL                  |5878628895   |Japan           |\n",
      "|babec972-ffb3-4c56-99c3-e8e3855adf0f|Christina Sanchez|craigprice@example.org|4832368495   |Haiti           |\n",
      "|d74c33bd-69d9-4718-9e00-d1895a41ddac|Thomas Brown     |vjohnson@example.org  |(276)903-7065|Pakistan        |\n",
      "|ff05ceba-f459-4714-a252-e03198d9934c|Lindsey Bradford |kathryn50@example.net |NULL         |Marshall Islands|\n",
      "|f20755f6-8481-4904-afe6-504451ceded5|John Boyer       |jennifer15@example.org|(749)644-5721|New Caledonia   |\n",
      "+------------------------------------+-----------------+----------------------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+------------+-----------+-----+\n",
      "|Product_ID|Product_Name|Category   |Price|\n",
      "+----------+------------+-----------+-----+\n",
      "|1         |him         |Home       |411  |\n",
      "|2         |why         |Electronics|333  |\n",
      "|3         |strategy    |Home       |645  |\n",
      "|4         |interview   |NULL       |290  |\n",
      "|5         |though      |Electronics|1738 |\n",
      "+----------+------------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------------------+------------------------------------+----------+----------+------+------------------------------------+\n",
      "|Transaction_ID                      |Customer_ID                         |Product_ID|Date      |Amount|Sales_Rep_ID                        |\n",
      "+------------------------------------+------------------------------------+----------+----------+------+------------------------------------+\n",
      "|31190b6c-54cd-4cdb-a89e-7d8633b386df|d7522cab-f713-4c74-9d32-fadba3a20a85|48        |2024-04-01|234.0 |8aede4f3-bd43-45a7-9edc-5d69a2645fa1|\n",
      "|05d4bcca-dc8e-405c-81aa-d2bfc2acbc00|dae0689d-0c38-440c-b921-fe2413c3df3b|3         |2024-03-28|119.0 |c48b42a3-544b-4542-8528-4208e6f80b46|\n",
      "|9b6e01be-1814-41e0-bd7d-3ae2f845f5bf|075005e4-bb34-4965-bb28-fd100b10c45c|26        |2024-06-09|240.0 |d7558c8d-5355-48e7-8b88-30ab41ff2b8f|\n",
      "|46bd8f7c-20e0-405b-a1fe-b78e2b9a6813|ec910504-7738-463c-b269-1b1617b09ad1|37        |2024-03-24|451.0 |d0fa26a0-d161-4976-ae9a-9c5f74fc103c|\n",
      "|f02dded4-81bf-4310-8715-65e4fa9e1042|84441f52-77e5-450b-97d2-80f64c5c4fb4|4         |2024-05-29|78.0  |8aede4f3-bd43-45a7-9edc-5d69a2645fa1|\n",
      "+------------------------------------+------------------------------------+----------+----------+------+------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "|Interaction_ID                      |Customer_ID                         |Interaction_Date|Interaction_Type|Issue_Resolved|\n",
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "|5a367006-c47a-4728-a042-c4c6ddd9ee3e|4166b61a-cb6d-4190-ac0b-df812c0308ff|2024-08-07      |NULL            |true          |\n",
      "|544d6348-cc9c-4c36-9fbe-eaf7e845e682|cc9a2115-b197-4a2b-9a6a-0fb2da5a0c73|2024-01-30      |Email           |true          |\n",
      "|b1739e10-0690-4ec6-9a6b-147127c0f388|1f22e566-13a0-4758-b42c-b0f6e997d46c|2024-06-27      |Chat            |true          |\n",
      "|3d9dcd53-7edd-4672-bfcf-a03b430b2935|dae0689d-0c38-440c-b921-fe2413c3df3b|2024-07-20      |Email           |false         |\n",
      "|2861521b-43ba-45d9-a0bd-9ee5c2e3edf4|c1e7a24a-0cc4-4a3e-8cf1-1e3487076ef4|2024-05-31      |Email           |false         |\n",
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------------------------+-----------------+-------------+------------+--------------+\n",
      "|Sales_Rep_ID                        |Name             |Region       |Sales_Target|Sales_Achieved|\n",
      "+------------------------------------+-----------------+-------------+------------+--------------+\n",
      "|0437b05a-9628-43f9-ac07-0b9a0dc96dcd|Brittany Taylor  |California   |41135       |14037.0       |\n",
      "|4daeb6af-d7e9-4f99-91b3-6c912f45b740|Mitchell Williams|New Hampshire|32996       |21461.0       |\n",
      "|f243144e-485f-4382-81ef-2a9a3c63f172|John Terry       |Kansas       |10385       |NULL          |\n",
      "|9c44ee81-8254-45e1-af23-a4608ceb126c|Carolyn Miller   |Arizona      |23754       |17149.0       |\n",
      "|3e97b5d8-933a-4860-bce7-2398af6c5613|Antonio Sparks   |Washington   |27101       |36413.0       |\n",
      "+------------------------------------+-----------------+-------------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load data from CSV files into DataFrames using PySpark\n",
    "logger.info(\"Step 1: Loading data from CSV files into DataFrames...\")\n",
    "\n",
    "# Define a list of tuples mapping file names to DataFrame variables\n",
    "file_names = [\n",
    "    (\"customers_df\", \"Dataset/customers.csv\"),\n",
    "    (\"products_df\", \"Dataset/products.csv\"),\n",
    "    (\"transactions_df\", \"Dataset/transactions.csv\"),\n",
    "    (\"interactions_df\", \"Dataset/interactions.csv\"),\n",
    "    (\"sales_team_df\", \"Dataset/sales_team.csv\")\n",
    "]\n",
    "\n",
    "# Load all DataFrames using a loop\n",
    "dfs = {}\n",
    "for df_var, file in file_names:\n",
    "    dfs[df_var] = spark.read.csv(file, header=True, inferSchema=True)\n",
    "\n",
    "# Assign DataFrames to their respective variables\n",
    "customers_df = dfs[\"customers_df\"]\n",
    "products_df = dfs[\"products_df\"]\n",
    "transactions_df = dfs[\"transactions_df\"]\n",
    "interactions_df = dfs[\"interactions_df\"]\n",
    "sales_team_df = dfs[\"sales_team_df\"]\n",
    "\n",
    "# Step 2: Initial Validation - Display records for each DataFrame\n",
    "logger.info(\"Step 2: Displaying initial records for validation...\")\n",
    "\n",
    "# Loop through the DataFrames to display the first 5 records\n",
    "for df_var, _ in file_names:\n",
    "    logger.info(f\"{df_var.replace('_df', '').capitalize()} DataFrame:\")\n",
    "    dfs[df_var].show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Customers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:54,322 - DataCleaningLogger - INFO - \u001b[92mStep 1: Counting missing values in each column before filling them...\u001b[0m\n",
      "2024-09-02 16:49:54,434 - DataCleaningLogger - INFO - \u001b[92mStep 2: Checking for duplicates in each column before dropping them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+-----+-------+\n",
      "|Customer_ID|Name|Email|Phone|Country|\n",
      "+-----------+----+-----+-----+-------+\n",
      "|          0|   0|   52|   52|      0|\n",
      "+-----------+----+-----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:55,524 - DataCleaningLogger - INFO - \u001b[92mStep 3: Formatting and cleaning phone numbers...\u001b[0m\n",
      "2024-09-02 16:49:55,664 - DataCleaningLogger - INFO - \u001b[92mStep 4: Checking and removing duplicate records based on 'Customer_ID'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|     Column|Duplicate_Count|\n",
      "+-----------+---------------+\n",
      "|Customer_ID|             25|\n",
      "|       Name|             26|\n",
      "|      Email|             22|\n",
      "|      Phone|             23|\n",
      "|    Country|            157|\n",
      "+-----------+---------------+\n",
      "\n",
      "Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:55,976 - DataCleaningLogger - INFO - \u001b[92mStep 5: Formatting and cleaning Email...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records before dropping: 25\n",
      "Number of duplicate records after dropping: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:56,230 - DataCleaningLogger - INFO - \u001b[92mStep 5: Filling missing values in 'Email' and 'Phone' columns...\u001b[0m\n",
      "2024-09-02 16:49:56,396 - DataCleaningLogger - INFO - \u001b[92mStep 6: Capitalizing the first letter of each word in the 'Name' and 'Country' columns...\u001b[0m\n",
      "2024-09-02 16:49:56,407 - DataCleaningLogger - INFO - \u001b[92mStep 7: Counting missing values in each column after filling them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All non-null emails are valid.\n",
      "Completed.\n",
      "+------------------------------------+-----------------+----------------------+---------------+-----------+\n",
      "|Customer_ID                         |Name             |Email                 |Phone          |Country    |\n",
      "+------------------------------------+-----------------+----------------------+---------------+-----------+\n",
      "|003ca69a-991c-4c11-899a-51bb7365499d|Erica Diaz       |hurleyanna@example.com|+964-8545044941|Iraq       |\n",
      "|006af455-013b-4c09-a6df-15ca3d41010f|Jason Jackson    |dylanduran@example.com|+63-2484285464 |Philippines|\n",
      "|00fc38f7-b5c7-465c-839b-a55185f2635f|Heather Schneider|larajohn@example.org  |+264-7935879728|Namibia    |\n",
      "|015cc4e1-5cf8-441c-80ad-ca3536c53e9a|Matthew Wilson   |timothyho@example.org |+357-8883554148|Cyprus     |\n",
      "|01d13428-d511-4c3d-90c7-b5624931ff48|Robert Contreras |lydia12@example.com   |unknown        |Swaziland  |\n",
      "+------------------------------------+-----------------+----------------------+---------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:56,603 - DataCleaningLogger - INFO - \u001b[92mStep 8: Checking for duplicates in each column after dropping them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+-----+-------+\n",
      "|Customer_ID|Name|Email|Phone|Country|\n",
      "+-----------+----+-----+-----+-------+\n",
      "|          0|   0|    0|    0|      0|\n",
      "+-----------+----+-----+-----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:57,638 - DataCleaningLogger - INFO - \u001b[92mStep 9: Exporting the cleaned data to 'cleaned_customers.csv'...\u001b[0m\n",
      "2024-09-02 16:49:57,746 - DataCleaningLogger - INFO - \u001b[92mData cleaning and export completed successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|     Column|Duplicate_Count|\n",
      "+-----------+---------------+\n",
      "|Customer_ID|              0|\n",
      "|       Name|              1|\n",
      "|      Email|              1|\n",
      "|      Phone|              1|\n",
      "|    Country|            151|\n",
      "+-----------+---------------+\n",
      "\n",
      "Number of records after cleaning: 500\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Count missing (null) values for each column before filling them\n",
    "logger.info(\"Step 1: Counting missing values in each column before filling them...\")\n",
    "missing_values_before = count_missing_values(customers_df)\n",
    "missing_values_before.show()\n",
    "\n",
    "# Step 2: Count duplicates before dropping them\n",
    "logger.info(\"Step 2: Checking for duplicates in each column before dropping them...\")\n",
    "duplicate_count = count_duplicates_per_column(customers_df)\n",
    "duplicate_count.show()\n",
    "\n",
    "# Step 3: Format and clean phone numbers\n",
    "logger.info(\"Step 3: Formatting and cleaning phone numbers...\")\n",
    "cleaned_customers_df = process_phone_numbers(customers_df, \"Dataset/countries.csv\")\n",
    "print(\"Completed.\")\n",
    "\n",
    "# Step 4: Drop duplicates based on 'Customer_ID' if any are found\n",
    "logger.info(\"Step 4: Checking and removing duplicate records based on 'Customer_ID'...\")\n",
    "cleaned_customers_df = drop_duplicates(cleaned_customers_df, \"Customer_ID\")\n",
    "\n",
    "# Step 5: Format and clean Email\n",
    "logger.info(\"Step 5: Formatting and cleaning Email...\")\n",
    "cleaned_customers_df = validate_emails(cleaned_customers_df, \"Email\")\n",
    "print(\"Completed.\")\n",
    "\n",
    "# Step 5: Fill missing values in 'Email' and 'Phone' columns\n",
    "logger.info(\"Step 5: Filling missing values in 'Email' and 'Phone' columns...\")\n",
    "cleaned_customers_df = fill_missing_values(cleaned_customers_df, {'Email': 'unknown', 'Phone': 'unknown'})\n",
    "cleaned_customers_df.show(5, truncate=False)\n",
    "\n",
    "# Step 6: Capitalize the first letter of the first and last names in the 'Name' and 'Country' columns\n",
    "logger.info(\"Step 6: Capitalizing the first letter of each word in the 'Name' and 'Country' columns...\")\n",
    "cleaned_customers_df = capitalize_columns(cleaned_customers_df, [\"Name\", \"Country\"])\n",
    "print(\"Completed.\")\n",
    "\n",
    "# Step 7: Cross-validation - Count missing values again after filling them\n",
    "logger.info(\"Step 7: Counting missing values in each column after filling them...\")\n",
    "missing_values_after = count_missing_values(cleaned_customers_df)\n",
    "missing_values_after.show()\n",
    "\n",
    "# Step 8: Count duplicates after dropping them\n",
    "logger.info(\"Step 8: Checking for duplicates in each column after dropping them...\")\n",
    "duplicate_count_after = count_duplicates_per_column(cleaned_customers_df)\n",
    "duplicate_count_after.show()\n",
    "\n",
    "# Step 9: Export the cleaned data to a CSV file\n",
    "logger.info(\"Step 9: Exporting the cleaned data to 'cleaned_customers.csv'...\")\n",
    "# save_df_to_csv(cleaned_customers_df, \"Cleaned_data/cleaned_customers.csv\")\n",
    "\n",
    "# Display the count of records after phone number processing\n",
    "record_count_after_cleaning = cleaned_customers_df.count()\n",
    "print(f\"Number of records after cleaning: {record_count_after_cleaning}\")\n",
    "\n",
    "logger.info(\"Data cleaning and export completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Products Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:57,764 - DataCleaningLogger - INFO - \u001b[92mStep 1: Counting missing values in each column before filling them...\u001b[0m\n",
      "2024-09-02 16:49:57,865 - DataCleaningLogger - INFO - \u001b[92mStep 2: Checking for duplicates in each column before dropping them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+-----+\n",
      "|Product_ID|Product_Name|Category|Price|\n",
      "+----------+------------+--------+-----+\n",
      "|         0|           0|       3|    0|\n",
      "+----------+------------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:58,630 - DataCleaningLogger - INFO - \u001b[92mStep 3: Checking and removing duplicate records based on 'Customer_ID'...\u001b[0m\n",
      "2024-09-02 16:49:58,818 - DataCleaningLogger - INFO - \u001b[92mStep 4: Filling missing values in 'Email' and 'Phone' columns...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|      Column|Duplicate_Count|\n",
      "+------------+---------------+\n",
      "|  Product_ID|              1|\n",
      "|Product_Name|              2|\n",
      "|    Category|              5|\n",
      "|       Price|              2|\n",
      "+------------+---------------+\n",
      "\n",
      "Number of duplicate records before dropping: 1\n",
      "Number of duplicate records after dropping: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:58,905 - DataCleaningLogger - INFO - \u001b[92mStep 5: Capitalizing the first letter of each word in the 'Product_Name' and 'Category' columns...\u001b[0m\n",
      "2024-09-02 16:49:58,913 - DataCleaningLogger - INFO - \u001b[92mStep 6: Counting missing values in each column after filling them...\u001b[0m\n",
      "2024-09-02 16:49:59,037 - DataCleaningLogger - INFO - \u001b[92mStep 7: Checking for duplicates in each column after dropping them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+-----+\n",
      "|Product_ID|Product_Name|Category     |Price|\n",
      "+----------+------------+-------------+-----+\n",
      "|1         |him         |Home         |411  |\n",
      "|2         |why         |Electronics  |333  |\n",
      "|3         |strategy    |Home         |645  |\n",
      "|4         |interview   |Uncategorized|290  |\n",
      "|5         |though      |Electronics  |1738 |\n",
      "+----------+------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Completed.\n",
      "+----------+------------+--------+-----+\n",
      "|Product_ID|Product_Name|Category|Price|\n",
      "+----------+------------+--------+-----+\n",
      "|         0|           0|       0|    0|\n",
      "+----------+------------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:49:59,919 - DataCleaningLogger - INFO - \u001b[92mStep 8: Replacing negative or zero prices with the average price...\u001b[0m\n",
      "2024-09-02 16:49:59,992 - DataCleaningLogger - INFO - \u001b[92mStep 9: Exporting the cleaned data to 'cleaned_products.csv'...\u001b[0m\n",
      "2024-09-02 16:50:00,076 - DataCleaningLogger - INFO - \u001b[92mData cleaning and export completed successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|      Column|Duplicate_Count|\n",
      "+------------+---------------+\n",
      "|  Product_ID|              0|\n",
      "|Product_Name|              1|\n",
      "|    Category|              5|\n",
      "|       Price|              1|\n",
      "+------------+---------------+\n",
      "\n",
      "completed.\n",
      "Number of records after cleaning: 50\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Count missing (null) values for each column before filling them\n",
    "logger.info(\"Step 1: Counting missing values in each column before filling them...\")\n",
    "missing_values_before = count_missing_values(products_df)\n",
    "missing_values_before.show()\n",
    "\n",
    "# Step 2: Count duplicates before dropping them\n",
    "logger.info(\"Step 2: Checking for duplicates in each column before dropping them...\")\n",
    "duplicate_count = count_duplicates_per_column(products_df)\n",
    "duplicate_count.show()\n",
    "\n",
    "# Step 3: Drop duplicates based on 'Customer_ID' if any are found\n",
    "logger.info(\"Step 3: Checking and removing duplicate records based on 'Customer_ID'...\")\n",
    "cleaned_products_df = drop_duplicates(products_df, \"Product_ID\")\n",
    "\n",
    "# Step 4: Fill missing values in 'Email' and 'Phone' columns\n",
    "logger.info(\"Step 4: Filling missing values in 'Email' and 'Phone' columns...\")\n",
    "cleaned_products_df = fill_missing_values(cleaned_products_df, {\"Category\": \"Uncategorized\"})\n",
    "cleaned_products_df.show(5, truncate=False)\n",
    "\n",
    "# Step 5: Capitalize the first letter of the first and last names in the 'Name' and 'Country' columns\n",
    "logger.info(\"Step 5: Capitalizing the first letter of each word in the 'Product_Name' and 'Category' columns...\")\n",
    "cleaned_products_df = capitalize_columns(cleaned_products_df, [\"Product_Name\", \"Category\"])\n",
    "print(\"Completed.\")\n",
    "\n",
    "# Step 6: Cross-validation - Count missing values again after filling them\n",
    "logger.info(\"Step 6: Counting missing values in each column after filling them...\")\n",
    "missing_values_after = count_missing_values(cleaned_products_df)\n",
    "missing_values_after.show()\n",
    "\n",
    "# Step 7: Count duplicates after dropping them\n",
    "logger.info(\"Step 7: Checking for duplicates in each column after dropping them...\")\n",
    "duplicate_count_after = count_duplicates_per_column(cleaned_products_df)\n",
    "duplicate_count_after.show()\n",
    "\n",
    "# Step 8: Handle negative or zero prices by replacing with average price\n",
    "logger.info(\"Step 8: Replacing negative or zero prices with the average price...\")\n",
    "avg_price = products_df.agg({\"Price\": \"avg\"}).collect()[0][0]\n",
    "cleaned_products_df = cleaned_products_df.withColumn(\"Price\", when(col(\"Price\") <= 0, avg_price).otherwise(col(\"Price\")))\n",
    "print(\"completed.\")\n",
    "\n",
    "# Step 9: Export the cleaned data to a CSV file\n",
    "logger.info(\"Step 9: Exporting the cleaned data to 'cleaned_products.csv'...\")\n",
    "# save_df_to_csv(cleaned_customers_df, \"Cleaned_data/cleaned_products.csv\")\n",
    "\n",
    "# show duplicates\n",
    "# get_duplicate_data_per_column(cleaned_products_df)\n",
    "\n",
    "# Display the count of records after phone number processing\n",
    "record_count_after_cleaning = cleaned_products_df.count()\n",
    "print(f\"Number of records after cleaning: {record_count_after_cleaning}\")\n",
    "\n",
    "logger.info(\"Data cleaning and export completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transactions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:00,090 - DataCleaningLogger - INFO - \u001b[92mStep 1: Counting missing values in each column before filling them...\u001b[0m\n",
      "2024-09-02 16:50:00,216 - DataCleaningLogger - INFO - \u001b[92mStep 2: Calculating the mean value of the 'Amount' column...\u001b[0m\n",
      "2024-09-02 16:50:00,278 - DataCleaningLogger - INFO - \u001b[92mStep 3: Filling missing values in the 'Amount' column with the mean value...\u001b[0m\n",
      "2024-09-02 16:50:00,282 - DataCleaningLogger - INFO - \u001b[92mStep 4: Checking for duplicate records based on 'Transaction_ID'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+----+------+------------+\n",
      "|Transaction_ID|Customer_ID|Product_ID|Date|Amount|Sales_Rep_ID|\n",
      "+--------------+-----------+----------+----+------+------------+\n",
      "|             0|          0|         0|   0|    52|           0|\n",
      "+--------------+-----------+----------+----+------+------------+\n",
      "\n",
      "Mean value calculated for 'Amount': 282.78\n",
      "Number of duplicate records before dropping: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:00,533 - DataCleaningLogger - INFO - \u001b[92mStep 5: Ensuring 'Date' column is in YYYY-MM-DD format...\u001b[0m\n",
      "2024-09-02 16:50:00,540 - DataCleaningLogger - INFO - \u001b[92mStep 6: Validating dates present in Date column...\u001b[0m\n",
      "2024-09-02 16:50:00,663 - DataCleaningLogger - INFO - \u001b[92mStep 7: Counting missing values in each column after filling them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records after dropping: 0\n",
      "Step 1: Identifying future dates in 'Interaction_Date'...\n",
      "No future dates found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:00,849 - DataCleaningLogger - INFO - \u001b[92mStep 8: Correcting non-positive values in the 'Amount' column...\u001b[0m\n",
      "2024-09-02 16:50:00,859 - DataCleaningLogger - INFO - \u001b[92mStep 9: Displaying the cleaned 'transactions_df' DataFrame...\u001b[0m\n",
      "2024-09-02 16:50:00,984 - DataCleaningLogger - INFO - \u001b[92mStep 10: Saving the cleaned data to 'cleaned_transactions.csv'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------+----+------+------------+\n",
      "|Transaction_ID|Customer_ID|Product_ID|Date|Amount|Sales_Rep_ID|\n",
      "+--------------+-----------+----------+----+------+------------+\n",
      "|             0|          0|         0|   0|     0|           0|\n",
      "+--------------+-----------+----------+----+------+------------+\n",
      "\n",
      "+------------------------------------+------------------------------------+----------+----------+------+------------------------------------+\n",
      "|Transaction_ID                      |Customer_ID                         |Product_ID|Date      |Amount|Sales_Rep_ID                        |\n",
      "+------------------------------------+------------------------------------+----------+----------+------+------------------------------------+\n",
      "|00212675-af49-4cbd-9766-4303572b7506|41b14a02-df69-4f06-bba9-a32a32d7c008|30        |2024-05-01|125.0 |61989b73-1807-414a-a75b-f194c0152dba|\n",
      "|003a04fd-3d61-4af1-bf78-64b4d8bec34b|2fbd8187-90f1-409e-89ff-3ff4cae288e6|14        |2024-01-23|50.0  |5184e796-6c9f-4019-907b-6e9e838430f3|\n",
      "|007fa68d-e211-47af-85dc-e02f7477b5a9|006af455-013b-4c09-a6df-15ca3d41010f|43        |2024-02-14|357.0 |33ea8c7d-1b0a-4be7-9cff-d35ea357325f|\n",
      "|00946fd1-cc54-4671-936d-7f6f25723e62|552651e9-3ff6-4c64-bdc5-863a8c65c5b2|28        |2024-01-19|448.0 |2f1183af-7ef6-498a-a851-91820338ff67|\n",
      "|00b3f73b-52d6-46ce-b71e-08522851906d|2dc3fe00-cfd8-423c-bf2a-8421f90fcc76|27        |2024-05-19|210.0 |05cc8513-8a8a-4598-862a-218d1093ae26|\n",
      "|01489450-6ce8-43fa-83e2-78ceddca1f45|2d939a24-9c21-491d-890e-9b480713eb0e|25        |2024-07-17|448.0 |e48e19e2-3978-4330-ac3b-4e3217312d72|\n",
      "|0197913a-0fe3-4bcc-a518-e510b22591c4|51983bd3-03a4-42c8-b784-e978d8771d0f|10        |2024-02-17|115.0 |d07336a6-e21c-45d9-bc59-bb63382649bc|\n",
      "|01a5dfc4-143d-4609-958d-881b80323ade|7622c988-b135-4108-8c85-f99b453b1f9d|43        |2024-03-06|282.78|1f463865-908f-453f-8c86-5832f423dba6|\n",
      "|01e4b0f2-d7b9-4c5c-a8aa-c5deb01c8876|df1d006a-dcec-4ae8-902e-76ed5de0c0f5|19        |2024-01-16|334.0 |ab37df34-610e-4c16-956b-4f2e38cb60bf|\n",
      "|01fe6c62-b0c2-4e1c-8708-b2aed531ab9b|d63527d7-7e3a-477a-9647-7ea1d9bdb87f|38        |2024-03-12|384.0 |0437b05a-9628-43f9-ac07-0b9a0dc96dcd|\n",
      "|02aa6340-fc49-471a-9b4e-d051e5d0dfe5|6f632742-7f7b-48e7-9c3a-4c5e927f3c2e|27        |2024-02-19|118.0 |e436fc49-a67b-4eda-bab9-a19a0b31a3e2|\n",
      "|02b94cfa-e90b-45df-aba8-7d3e27c1f2fd|f00eadce-4988-43df-a514-93ddc3b999e6|2         |2024-03-25|410.0 |05cc8513-8a8a-4598-862a-218d1093ae26|\n",
      "|02befddd-d767-4a1b-a85d-270b2e71d933|9fde07a1-09d5-4c61-bcbd-4ac93e845af9|15        |2024-06-11|402.0 |0437b05a-9628-43f9-ac07-0b9a0dc96dcd|\n",
      "|02e30fad-7883-4af2-b631-464820784e0b|5f3977f9-82ae-42d7-97b5-114e8274b349|41        |2024-03-22|190.0 |4daeb6af-d7e9-4f99-91b3-6c912f45b740|\n",
      "|034c672d-c470-4b22-a4f5-b72915242506|61a98216-3f7a-4a96-b561-172f46606985|30        |2024-02-02|282.78|9dd03ac8-7cac-493d-bcd1-06ca9a66ca33|\n",
      "|03959945-d4c2-4bcd-9e8d-c93a7aa32c8d|b8eaa8af-fe31-4f60-a4d3-2037756d4e56|6         |2024-06-06|253.0 |e436fc49-a67b-4eda-bab9-a19a0b31a3e2|\n",
      "|03edb7d5-a2a8-44fc-89dc-b1a0b97564e4|bc7a1591-33bf-464d-ae17-fdc622dd31d4|1         |2024-07-04|192.0 |e436fc49-a67b-4eda-bab9-a19a0b31a3e2|\n",
      "|0522c5e1-7252-41d1-873e-f988a2699178|59f3019e-8d7e-4dc3-b23f-c1dab63c5f4e|25        |2024-05-12|226.0 |d07336a6-e21c-45d9-bc59-bb63382649bc|\n",
      "|0525dba9-a9c3-4bb4-8a0a-7a46a780d2b3|e80d8281-f41f-4701-ac74-5375a3cc7abb|29        |2024-05-17|490.0 |13303e49-6260-4e49-93da-e1abfd37a692|\n",
      "|057e0294-2c4b-4faf-b6e7-ea31941728da|9cfec88c-9489-408d-a297-3fa9bfd32674|19        |2024-04-21|342.0 |2f1183af-7ef6-498a-a851-91820338ff67|\n",
      "+------------------------------------+------------------------------------+----------+----------+------+------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:01,081 - DataCleaningLogger - INFO - \u001b[92mData cleaning and export completed successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after cleaning: 1000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Count missing values in each column before filling them\n",
    "logger.info(\"Step 1: Counting missing values in each column before filling them...\")\n",
    "missing_values_before = count_missing_values(transactions_df)\n",
    "missing_values_before.show()\n",
    "\n",
    "# Step 2: Calculate the mean value of the 'Amount' column\n",
    "logger.info(\"Step 2: Calculating the mean value of the 'Amount' column...\")\n",
    "mean_amount_row = transactions_df.agg(mean(\"Amount\")).collect()[0]\n",
    "mean_amount = mean_amount_row[0]\n",
    "print(f\"Mean value calculated for 'Amount': {mean_amount:.2f}\")\n",
    "\n",
    "# Step 3: Fill missing values in the 'Amount' column with the rounded mean value\n",
    "logger.info(\"Step 3: Filling missing values in the 'Amount' column with the mean value...\")\n",
    "cleaned_transactions_df = fill_missing_values(transactions_df, {\"Amount\": round(mean_amount, 2)})\n",
    "\n",
    "# Step 4: Check for duplicate records based on 'Transaction_ID'\n",
    "logger.info(\"Step 4: Checking for duplicate records based on 'Transaction_ID'...\")\n",
    "cleaned_transactions_df = drop_duplicates(cleaned_transactions_df, \"Transaction_ID\")\n",
    "\n",
    "# Step 5: Ensure 'Date' is in YYYY-MM-DD format\n",
    "logger.info(\"Step 5: Ensuring 'Date' column is in YYYY-MM-DD format...\")\n",
    "cleaned_transactions_df = cleaned_transactions_df.withColumn(\n",
    "    \"Date\", date_format(col(\"Date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Step 6: date validation \n",
    "logger.info(\"Step 6: Validating dates present in Date column...\")\n",
    "cleaned_transactions_df = date_validation(cleaned_transactions_df,\"Date\")\n",
    "\n",
    "# Step 7: Cross-verification after filling missing values in each column\n",
    "logger.info(\"Step 7: Counting missing values in each column after filling them...\")\n",
    "missing_values_after = count_missing_values(cleaned_transactions_df)\n",
    "missing_values_after.show()\n",
    "\n",
    "# Step 8: Correct inaccurate data (e.g., non-positive 'Amount')\n",
    "logger.info(\"Step 8: Correcting non-positive values in the 'Amount' column...\")\n",
    "corrected_transactions_df = cleaned_transactions_df.withColumn(\n",
    "    \"Amount\",\n",
    "    when(col(\"Amount\") <= 0, mean_amount).otherwise(col(\"Amount\"))\n",
    ")\n",
    "\n",
    "# Step 9: Display the cleaned DataFrame\n",
    "logger.info(\"Step 9: Displaying the cleaned 'transactions_df' DataFrame...\")\n",
    "corrected_transactions_df.show(truncate=False)\n",
    "\n",
    "# Step 10: Save the cleaned data to a new CSV\n",
    "logger.info(\"Step 10: Saving the cleaned data to 'cleaned_transactions.csv'...\")\n",
    "# save_df_to_csv(corrected_transactions_df, \"Cleaned_data/cleaned_transactions.csv\")\n",
    "\n",
    "# Display the count of records after phone number processing\n",
    "record_count_after_cleaning = cleaned_transactions_df.count()\n",
    "print(f\"Number of records after cleaning: {record_count_after_cleaning}\")\n",
    "logger.info(\"Data cleaning and export completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interactions Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:01,096 - DataCleaningLogger - INFO - \u001b[92mStep 1: Identifying missing values in each column before filling them...\u001b[0m\n",
      "2024-09-02 16:50:01,223 - DataCleaningLogger - INFO - \u001b[92mStep 2: Counting occurrences of each 'Interaction_Type' to find the most occurring type...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "|Interaction_ID|Customer_ID|Interaction_Date|Interaction_Type|Issue_Resolved|\n",
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "|             0|          0|               0|              67|             0|\n",
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:01,321 - DataCleaningLogger - INFO - \u001b[92mStep 3: Replacing null values in 'Interaction_Type' with the most occurring type...\u001b[0m\n",
      "2024-09-02 16:50:01,331 - DataCleaningLogger - INFO - \u001b[92mStep 4: Checking for duplicate records based on 'Interaction_ID'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most occurring 'Interaction_Type' is: Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:01,516 - DataCleaningLogger - INFO - \u001b[92mStep 5: Capitalizing the first letter of each word in the and 'Interaction_Type' columns...\u001b[0m\n",
      "2024-09-02 16:50:01,520 - DataCleaningLogger - INFO - \u001b[92mStep 6: Validating dates present in Interaction_Date column...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records before dropping: 32\n",
      "Number of duplicate records after dropping: 0\n",
      "Completed.\n",
      "Step 1: Identifying future dates in 'Interaction_Date'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:01,635 - DataCleaningLogger - INFO - \u001b[92mStep 7: Validating booleans present in Issue_Resolved column...\u001b[0m\n",
      "2024-09-02 16:50:01,777 - DataCleaningLogger - INFO - \u001b[92mStep 8: Identifying missing values in each column after filling them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No future dates found.\n",
      "Step 1: Identifying non-boolean values.. \n",
      "All values in 'Issue_Resolved' are valid booleans.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:01,970 - DataCleaningLogger - INFO - \u001b[92mStep 9: Displaying the cleaned 'interactions_df' DataFrame...\u001b[0m\n",
      "2024-09-02 16:50:02,049 - DataCleaningLogger - INFO - \u001b[92mStep 10: Saving the cleaned data to 'cleaned_interactions.csv'...\u001b[0m\n",
      "2024-09-02 16:50:02,148 - DataCleaningLogger - INFO - \u001b[92mData cleaning and export completed successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "|Interaction_ID|Customer_ID|Interaction_Date|Interaction_Type|Issue_Resolved|\n",
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "|             0|          0|               0|               0|             0|\n",
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "\n",
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "|Interaction_ID                      |Customer_ID                         |Interaction_Date|Interaction_Type|Issue_Resolved|\n",
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "|002debb8-7e9e-476c-85bc-dd3e8b7210ce|dfce9c24-71c4-4cd8-9b59-f34e48842979|2024-01-30      |Chat            |true          |\n",
      "|00571a5f-03ff-4615-85cf-cd540f2ccdad|bd56525d-e899-4312-9763-e79d3f6fa54c|2024-02-08      |Chat            |false         |\n",
      "|00960c9f-02c5-407c-8bb4-5d81747be2ea|f963092b-fa72-4017-bfa5-a7daec18bc46|2024-02-15      |Chat            |false         |\n",
      "|00f8180f-66fe-46a5-9d8c-caf7b97c23d1|0a1fae74-7af1-4032-8d72-0b0a14f9bd1b|2024-06-29      |Chat            |false         |\n",
      "|016b4c4f-4800-43d4-b6e8-f493fb01746d|b552b0ab-faf1-452b-a2e6-7b37a31aa881|2024-01-25      |Email           |true          |\n",
      "|017a1931-184f-4dd6-8ee0-a9a937a22ebc|5aa953b3-e4bd-4b14-be0b-9e895d39156c|2024-08-09      |Email           |false         |\n",
      "|01e76b25-0523-40a0-a8f4-96275c2eb940|c2622e7f-1cb5-4dc2-90d2-46fe480dcae1|2024-05-03      |Phone           |false         |\n",
      "|020a69b5-eaf5-41a1-9331-4f9a6cd56ddf|5613fae4-8955-454c-81d7-c4d0ce4a4cbc|2024-01-10      |Chat            |true          |\n",
      "|026d75a7-e60d-4567-9499-941150b5e679|36b57eed-b59e-4f00-957b-af98ddda3e77|2024-02-07      |Chat            |true          |\n",
      "|02c1f05a-d7e8-4a18-bd1f-9d84590ac506|e738d2c5-cd3e-48fc-b2d4-5c181e398120|2024-01-01      |Chat            |true          |\n",
      "|032c050a-b70b-4fd2-9f1a-c4050d1d53ce|fe887e53-e78f-4207-8e5e-73e38baf3e0d|2024-06-08      |Phone           |false         |\n",
      "|033625ce-674b-441c-a23b-72a803edd5e7|fdb19bbf-7ee4-4833-bba6-e9b22a898e39|2024-02-24      |Phone           |true          |\n",
      "|03419971-04a9-492e-98ba-9307be7a4b08|16c06854-b963-4325-bf44-295783569f40|2024-04-19      |Email           |true          |\n",
      "|0487f44d-b145-4b4a-99b3-1845ae1843f5|9ef75e2b-2517-4e6d-b752-ae890322233a|2024-07-26      |Chat            |true          |\n",
      "|04e95ab9-efd9-466e-bc18-039aa413efbd|500a7880-9ca6-40ce-9c21-65186d7f1eb8|2024-07-16      |Chat            |false         |\n",
      "|050ffc66-d04b-46e8-a693-464d04a68548|a672f866-e1e2-4212-9f00-c331c0606029|2024-01-05      |Chat            |false         |\n",
      "|0524b96a-f8b8-4094-894a-164f0ec86d65|a816cf2d-0c49-4c1c-9a05-64715fc11b29|2024-06-23      |Email           |true          |\n",
      "|05da4220-6efd-4603-a634-34256c6f1273|8e39546e-5d13-42c5-b956-84d02ba2e1b7|2024-02-18      |Chat            |true          |\n",
      "|05eeeff1-bc23-47ce-8473-933385f19b23|2d374acf-1c9c-412e-ac53-90e9892ac707|2024-02-03      |Email           |true          |\n",
      "|060175f0-7e84-4bc8-803e-08f1e4f02e59|d3a3c064-3115-4cd4-9d1d-3d2a77efa468|2024-08-08      |Phone           |false         |\n",
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of records after cleaning: 800\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify missing values in each column before filling them\n",
    "logger.info(\"Step 1: Identifying missing values in each column before filling them...\")\n",
    "missing_values_before = count_missing_values(interactions_df)\n",
    "missing_values_before.show()\n",
    "\n",
    "# Step 2: Count occurrences of each 'Interaction_Type' and identify the most occurring type\n",
    "logger.info(\"Step 2: Counting occurrences of each 'Interaction_Type' to find the most occurring type...\")\n",
    "type_counts = interactions_df.groupBy(\"Interaction_Type\").count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .first()\n",
    "\n",
    "# Extract the most occurring 'Interaction_Type'\n",
    "most_occurring_type = type_counts[\"Interaction_Type\"] if type_counts else None\n",
    "print(f\"The most occurring 'Interaction_Type' is: {most_occurring_type}\")\n",
    "\n",
    "# Step 3: Replace only null values in 'Interaction_Type' with the most occurring type\n",
    "logger.info(\"Step 3: Replacing null values in 'Interaction_Type' with the most occurring type...\")\n",
    "cleaned_interactions_df = interactions_df.withColumn(\n",
    "    \"Interaction_Type\",\n",
    "    when(col(\"Interaction_Type\").isNull(), lit(most_occurring_type))\n",
    "    .otherwise(col(\"Interaction_Type\"))\n",
    ")\n",
    "\n",
    "# Step 4: Check for duplicate records based on 'Interaction_ID'\n",
    "logger.info(\"Step 4: Checking for duplicate records based on 'Interaction_ID'...\")\n",
    "cleaned_interactions_df = drop_duplicates(cleaned_interactions_df, \"Interaction_ID\")\n",
    "\n",
    "# Step 5: Capitalize the first letter of the values in the 'Issue_Resolved' and 'Interaction_Type' columns\n",
    "logger.info(\"Step 5: Capitalizing the first letter of each word in the and 'Interaction_Type' columns...\")\n",
    "cleaned_interactions_df = capitalize_columns(cleaned_interactions_df, [ \"Interaction_Type\"])\n",
    "print(\"Completed.\")\n",
    "\n",
    "# Step 6: date validation \n",
    "logger.info(\"Step 6: Validating dates present in Interaction_Date column...\")\n",
    "cleaned_interactions_df = date_validation(cleaned_interactions_df,\"Interaction_Date\")\n",
    "\n",
    "# Step 7: boolean validation  \n",
    "logger.info(\"Step 7: Validating booleans present in Issue_Resolved column...\")\n",
    "cleaned_interactions_df = validate_boolean_values(cleaned_interactions_df,\"Issue_Resolved\")\n",
    "\n",
    "# Step 8: Cross-verification of missing values in each column after filling them\n",
    "logger.info(\"Step 8: Identifying missing values in each column after filling them...\")\n",
    "missing_values_after = count_missing_values(cleaned_interactions_df)\n",
    "missing_values_after.show()\n",
    "\n",
    "# Step 9: Display the cleaned 'interactions_df' DataFrame\n",
    "logger.info(\"Step 9: Displaying the cleaned 'interactions_df' DataFrame...\")\n",
    "cleaned_interactions_df.show(truncate=False)\n",
    "\n",
    "# Step 10: Save the cleaned data to a new CSV\n",
    "logger.info(\"Step 10: Saving the cleaned data to 'cleaned_interactions.csv'...\")\n",
    "# save_df_to_csv(cleaned_interactions_df, \"Cleaned_data/cleaned_interactions.csv\")\n",
    "\n",
    "# Display the count of records after phone number processing\n",
    "record_count_after_cleaning = cleaned_interactions_df.count()\n",
    "print(f\"Number of records after cleaning: {record_count_after_cleaning}\")\n",
    "logger.info(\"Data cleaning and export completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sales Team Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:02,164 - DataCleaningLogger - INFO - \u001b[92mStep 1: Identifying missing values in each column before filling them...\u001b[0m\n",
      "2024-09-02 16:50:02,267 - DataCleaningLogger - INFO - \u001b[92mStep 2: Handling missing values by filling with averages...\u001b[0m\n",
      "2024-09-02 16:50:02,327 - DataCleaningLogger - INFO - \u001b[92mStep 3: Checking for duplicate records based on 'Sales_Rep_ID'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+------------+--------------+\n",
      "|Sales_Rep_ID|Name|Region|Sales_Target|Sales_Achieved|\n",
      "+------------+----+------+------------+--------------+\n",
      "|           0|   0|     0|           0|             5|\n",
      "+------------+----+------+------------+--------------+\n",
      "\n",
      "Average value of sales achieved column is 21912.425531914894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:02,938 - DataCleaningLogger - INFO - \u001b[92mStep 6: Standardizing the format of 'Name' and 'Region' columns...\u001b[0m\n",
      "2024-09-02 16:50:02,949 - DataCleaningLogger - INFO - \u001b[92mStep 7: Identifying missing values in each column after filling them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate records before dropping: 2\n",
      "Number of duplicate records after dropping: 0\n",
      "Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:03,088 - DataCleaningLogger - INFO - \u001b[92mStep 8: Displaying the cleaned 'sales_team_df' DataFrame...\u001b[0m\n",
      "2024-09-02 16:50:03,226 - DataCleaningLogger - INFO - \u001b[92mStep 9: Saving the cleaned data to 'cleaned_sales_team.csv'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+------------+--------------+\n",
      "|Sales_Rep_ID|Name|Region|Sales_Target|Sales_Achieved|\n",
      "+------------+----+------+------------+--------------+\n",
      "|           0|   0|     0|           0|             0|\n",
      "+------------+----+------+------------+--------------+\n",
      "\n",
      "+------------------------------------+---------------------+--------------+------------+--------------+\n",
      "|Sales_Rep_ID                        |Name                 |Region        |Sales_Target|Sales_Achieved|\n",
      "+------------------------------------+---------------------+--------------+------------+--------------+\n",
      "|02acdbab-d149-4053-8c7a-871ba76f003e|Daniel Barber        |Arizona       |13651       |7296.0        |\n",
      "|0437b05a-9628-43f9-ac07-0b9a0dc96dcd|Brittany Taylor      |California    |41135       |14037.0       |\n",
      "|0468e32e-644b-491f-a3e1-741306f2c3f2|Dr. Wayne Spencer Dvm|Pennsylvania  |15593       |37653.0       |\n",
      "|05cc8513-8a8a-4598-862a-218d1093ae26|Pamela Pennington    |Idaho         |38399       |11569.0       |\n",
      "|06a9a95f-ae20-44ae-ad5d-d4bdf5f121d0|Sean Snyder          |Illinois      |19165       |11748.0       |\n",
      "|13303e49-6260-4e49-93da-e1abfd37a692|Edward Delacruz Ii   |Illinois      |15699       |29947.0       |\n",
      "|182f2a72-4271-4bbb-af12-abe8c23f6b80|Teresa Evans         |Oklahoma      |33311       |13239.0       |\n",
      "|1ce71e8b-2ac6-4939-bdca-476b9861362a|Alicia Johnston      |Colorado      |27703       |21295.0       |\n",
      "|1f463865-908f-453f-8c86-5832f423dba6|James Nicholson      |Vermont       |25306       |7319.0        |\n",
      "|2bad2c4e-1891-4fdf-921f-39712c19df58|Randy Bennett        |Mississippi   |21218       |11336.0       |\n",
      "|2f1183af-7ef6-498a-a851-91820338ff67|Justin Evans         |Oregon        |32612       |18354.0       |\n",
      "|33ea8c7d-1b0a-4be7-9cff-d35ea357325f|Anna Reid            |Arkansas      |30862       |38252.0       |\n",
      "|37455a0c-e36c-428f-b4cb-0ad5ce09f8b5|Linda Rogers         |Utah          |35183       |28521.0       |\n",
      "|3e97b5d8-933a-4860-bce7-2398af6c5613|Antonio Sparks       |Washington    |27101       |36413.0       |\n",
      "|440d8680-f45c-487d-9a53-1164227504ed|Emily Robinson       |Texas         |26985       |19751.0       |\n",
      "|459201da-22e7-417e-ba68-bc972bf96e81|Kimberly Green       |Utah          |19369       |29404.0       |\n",
      "|4daeb6af-d7e9-4f99-91b3-6c912f45b740|Mitchell Williams    |New Hampshire |32996       |21461.0       |\n",
      "|5184e796-6c9f-4019-907b-6e9e838430f3|Jasmine Davis        |Utah          |47445       |23227.0       |\n",
      "|5251ef10-fefe-4622-b4cf-b7d97bcce4fd|David Todd           |South Carolina|45521       |26803.0       |\n",
      "|5afc6a19-ffa5-43ef-8acd-d022f6837055|Rachael French       |Ohio          |40598       |21912.43      |\n",
      "+------------------------------------+---------------------+--------------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:50:03,295 - DataCleaningLogger - INFO - \u001b[92mData cleaning and export completed successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after cleaning: 50\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify Missing Values\n",
    "logger.info(\"Step 1: Identifying missing values in each column before filling them...\")\n",
    "missing_values_before = count_missing_values(sales_team_df)\n",
    "missing_values_before.show()\n",
    "\n",
    "# Step 2: Handle Missing Values\n",
    "logger.info(\"Step 2: Handling missing values by filling with averages...\")\n",
    "# Calculate average Sales_Achieved\n",
    "avg_sales_achieved = sales_team_df.select(avg(col(\"Sales_Achieved\"))).first()[0]\n",
    "print(f\"Average value of sales achieved column is {avg_sales_achieved}\")\n",
    "\n",
    "# Fill missing Sales_Achieved with the average\n",
    "sales_team_cleaned_df = fill_missing_values(sales_team_df,{\"Sales_Achieved\": round(avg_sales_achieved,2)})\n",
    "\n",
    "# Step 3: Check for Duplicate Values\n",
    "logger.info(\"Step 3: Checking for duplicate records based on 'Sales_Rep_ID'...\")\n",
    "duplicate_count_before = count_duplicates_per_column(sales_team_cleaned_df)\n",
    "\n",
    "# Step 4: Drop Duplicates if they exist\n",
    "sales_team_cleaned_df = drop_duplicates(sales_team_cleaned_df, \"Sales_Rep_ID\")\n",
    "\n",
    "# Step 6: Standardize Formats\n",
    "logger.info(\"Step 6: Standardizing the format of 'Name' and 'Region' columns...\")\n",
    "sales_team_cleaned_df = capitalize_columns(sales_team_cleaned_df, [ \"Name\", \"Region\"])\n",
    "print(\"Completed.\")\n",
    "\n",
    "# Step 7: Cross-verification of missing values in each column after filling them\n",
    "logger.info(\"Step 7: Identifying missing values in each column after filling them...\")\n",
    "missing_values_after = count_missing_values(sales_team_cleaned_df)\n",
    "missing_values_after.show()\n",
    "\n",
    "# Step 8: Display the cleaned 'sales_team_df' DataFrame\n",
    "logger.info(\"Step 8: Displaying the cleaned 'sales_team_df' DataFrame...\")\n",
    "sales_team_cleaned_df.show(truncate=False)\n",
    "\n",
    "# Step 9: Save the cleaned data to a new CSV\n",
    "logger.info(\"Step 9: Saving the cleaned data to 'cleaned_sales_team.csv'...\")\n",
    "# save_df_to_csv(sales_team_cleaned_df, \"Cleaned_data/cleaned_sales_team.csv\")\n",
    "\n",
    "# Display the count of records after phone number processing\n",
    "record_count_after_cleaning = sales_team_cleaned_df.count()\n",
    "print(f\"Number of records after cleaning: {record_count_after_cleaning}\")\n",
    "logger.info(\"Data cleaning and export completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
