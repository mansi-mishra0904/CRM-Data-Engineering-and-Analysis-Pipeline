{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/02 05:27:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, sum, lit, regexp_replace, initcap, when, mean, date_format, avg, concat,max as spark_max, current_date\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "from email_validator import validate_email, EmailNotValidError\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define a function to print in green\n",
    "def print_green(text):\n",
    "    print(f\"\\033[92m{text}\\033[0m\")\n",
    "\n",
    "# Step 1: Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"CRM_Data_Validation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to print in green\n",
    "def print_green(text):\n",
    "    print(f\"\\033[92m{text}\\033[0m\")\n",
    "\n",
    "def count_duplicates_per_column(df):\n",
    "    \"\"\"\n",
    "    count_duplicates_per_column function counts the number of duplicate values for each column in the DataFrame\n",
    "    and returns a DataFrame summarizing these counts.\n",
    "\n",
    "    This function groups the DataFrame by each column and counts occurrences of each value. It then filters\n",
    "    to count only those values that appear more than once, resulting in a count of duplicate values per column.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input Spark DataFrame containing potential duplicate values in its columns.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A Spark DataFrame where each row represents a column from the original DataFrame and the count of\n",
    "               duplicate values in that column.\n",
    "\n",
    "    Example:\n",
    "    ----------\n",
    "    Count duplicates in the customers_df DataFrame\n",
    "    duplicate_counts_df = count_duplicates_per_column(customers_df)\n",
    "    duplicate_counts_df.show()\n",
    "    \n",
    "    Output:\n",
    "    ----------\n",
    "    +-------------+-----------------+\n",
    "    | Column      | Duplicate_Count |\n",
    "    +-------------+-----------------+\n",
    "    | Customer_ID | 5               |\n",
    "    | Email       | 3               |\n",
    "    +-------------+-----------------+\n",
    "    \"\"\"\n",
    "    duplicate_counts = {}\n",
    "    for column in df.columns:\n",
    "        # Count the number of duplicate values in each column\n",
    "        duplicate_count = df.groupBy(col(column)).count().filter(col(\"count\") > 1).count()\n",
    "        duplicate_counts[column] = duplicate_count\n",
    "    # Convert the dictionary to a DataFrame for easier visualization\n",
    "    duplicate_counts_df = spark.createDataFrame(list(duplicate_counts.items()), [\"Column\", \"Duplicate_Count\"])\n",
    "    return duplicate_counts_df\n",
    "\n",
    "def get_duplicate_data_per_column(df):\n",
    "    \"\"\"\n",
    "    get_duplicate_data_per_column function identifies and retrieves rows with duplicate values based on each column\n",
    "    in the DataFrame and returns a dictionary of DataFrames containing the duplicate rows for each column.\n",
    "\n",
    "    This function groups the DataFrame by each column and identifies duplicates. It then joins the original DataFrame\n",
    "    with these duplicate groups to extract the full rows that have duplicate values.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input Spark DataFrame from which duplicate rows are to be identified.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where the keys are column names and the values are DataFrames containing rows with duplicate\n",
    "          values for that specific column.\n",
    "\n",
    "    Example:\n",
    "    ----------\n",
    "    Retrieve and display duplicate rows based on each column in cleaned_customers_df\n",
    "    duplicate_data = get_duplicate_data_per_column(cleaned_customers_df)\n",
    "    for column, duplicates in duplicate_data.items():\n",
    "        print(f\"Duplicate data based on column: {column}\")\n",
    "        duplicates.show(truncate=False)\n",
    "    \n",
    "    Output:\n",
    "    ----------\n",
    "    Duplicate data based on column: Customer_ID\n",
    "    +-----------+-----------+---------------------+\n",
    "    | Customer_ID | Name      | Email               |\n",
    "    +-----------+-----------+---------------------+\n",
    "    | 123       | Alice     | alice@example.com   |\n",
    "    | 123       | Alice     | alice@example.com   |\n",
    "    +-----------+-----------+---------------------+\n",
    "    \"\"\"\n",
    "    duplicate_data = {}\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # Find duplicate rows based on the specific column\n",
    "        duplicate_rows = df.groupBy(col(column)).count().filter(col(\"count\") > 1)\n",
    "        \n",
    "        # Join the duplicate rows with the original DataFrame to get full duplicate data\n",
    "        duplicates = df.join(duplicate_rows, on=column, how='inner')\n",
    "        \n",
    "        # Store the duplicates in the dictionary\n",
    "        duplicate_data[column] = duplicates\n",
    "\n",
    "        # Show duplicate data for each column\n",
    "    for column, duplicates in duplicate_data.items():\n",
    "        print(f\"Duplicate data based on column: {column}\")\n",
    "        duplicates.show(5,truncate=False)\n",
    "     \n",
    "\n",
    "\n",
    "def drop_duplicates(df: DataFrame, key_column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    This function checks for duplicate records in a specified key column,\n",
    "    drops duplicates if any are found, and returns the cleaned DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The input DataFrame to check for duplicates.\n",
    "    key_column : str\n",
    "        The column name based on which duplicates should be identified and removed.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    DataFrame\n",
    "        The cleaned DataFrame with duplicates removed, if any were found.\n",
    "\n",
    "        \n",
    "    Sample Input:\n",
    "    ----------\n",
    "    Input DataFrame:\n",
    "    +---+-----+----+\n",
    "    | ID| Name| Age|\n",
    "    +---+-----+----+\n",
    "    |  1| John|  25|\n",
    "    |  2| Jane|  30|\n",
    "    |  1| John|  25|\n",
    "    |  3| Mike|  35|\n",
    "    +---+-----+----+\n",
    "\n",
    "    key_column: 'ID'\n",
    "\n",
    "    Sample Output:\n",
    "    ----------\n",
    "    Cleaned DataFrame:\n",
    "    +---+-----+----+\n",
    "    | ID| Name| Age|\n",
    "    +---+-----+----+\n",
    "    |  1| John|  25|\n",
    "    |  2| Jane|  30|\n",
    "    |  3| Mike|  35|\n",
    "    +---+-----+----+\n",
    "    \"\"\"\n",
    "    # Count duplicates before dropping\n",
    "    duplicate_count_before = df.groupBy(key_column).count().filter(\"count > 1\").count()\n",
    "    print(f\"Number of duplicate records before dropping: {duplicate_count_before}\")\n",
    "\n",
    "    # Drop duplicates if any are found\n",
    "    if duplicate_count_before > 0:\n",
    "        cleaned_df = df.dropDuplicates([key_column])\n",
    "        duplicate_count_after = cleaned_df.groupBy(key_column).count().filter(\"count > 1\").count()\n",
    "        print(f\"Number of duplicate records after dropping: {duplicate_count_after}\")\n",
    "    else:\n",
    "        cleaned_df = df\n",
    "        print(\"No duplicates found.\")\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "def capitalize_columns(df: DataFrame, columns: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Capitalizes the first letter of each word in the specified columns of a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The input DataFrame whose columns need to be capitalized.\n",
    "    columns : list\n",
    "        A list of column names to apply the capitalization.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    DataFrame\n",
    "        The DataFrame with the specified columns capitalized.\n",
    "\n",
    "      \n",
    "    Sample Input:\n",
    "    ----------\n",
    "    Input DataFrame:\n",
    "    +---+------------+-------------+\n",
    "    | ID|  Name      |   City      |\n",
    "    +---+------------+-------------+\n",
    "    |  1| john doe   | new york    |\n",
    "    |  2| mike brown | chicago     |\n",
    "    +---+------------+-------------+\n",
    " \n",
    "    columns: ['Name', 'City']\n",
    "\n",
    "    Sample Output:\n",
    "    ----------\n",
    "    Output DataFrame:\n",
    "    +---+-----------+-------------+\n",
    "    | ID|    Name   |     City    |\n",
    "    +---+-----------+-------------+\n",
    "    |  1| John Doe  | New York    |\n",
    "    |  2| Mike Brown| Chicago     |\n",
    "    +---+-----------+-------------+\n",
    "\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        df = df.withColumn(column, initcap(col(column)))\n",
    "    return df\n",
    "\n",
    "def count_missing_values(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Counts the number of missing (null) values in each column of the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The input DataFrame for which missing values need to be counted.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    DataFrame\n",
    "        A DataFrame containing the count of missing values for each column.\n",
    "\n",
    "    Sample Input:\n",
    "    ----------\n",
    "    Input DataFrame:\n",
    "    +---+------+-----+\n",
    "    | ID| Name | Age |\n",
    "    +---+------+-----+\n",
    "    |  1| John |  25 |\n",
    "    |  2| null |  30 |\n",
    "    |  3| Mike | null|\n",
    "    |  4| null | null|\n",
    "    +---+------+-----+\n",
    "\n",
    "    Sample Output:\n",
    "    ----------\n",
    "    Output DataFrame:\n",
    "    +---+----+---+\n",
    "    | ID|Name|Age|\n",
    "    +---+----+---+\n",
    "    |  0|   2|  2|\n",
    "    +---+----+---+\n",
    "    \n",
    "    \"\"\"\n",
    "    missing_values_df = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "    return missing_values_df\n",
    "\n",
    "def fill_missing_values(df: DataFrame, fill_values: dict) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Fills missing (null) values in the specified columns with the provided default values.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The input DataFrame in which missing values need to be filled.\n",
    "    fill_values : dict\n",
    "        A dictionary where keys are column names and values are the default values \n",
    "        to fill in for missing data in those columns.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    DataFrame\n",
    "        The DataFrame with missing values filled according to the specified default values.\n",
    "\n",
    "    Sample Input:\n",
    "    ----------\n",
    "    Input DataFrame:\n",
    "    +---+------+-----+\n",
    "    | ID| Name | Age |\n",
    "    +---+------+-----+\n",
    "    |  1| John |  25 |\n",
    "    |  2| null |  30 |\n",
    "    |  3| Mike | null|\n",
    "    |  4| null | null|\n",
    "    +---+------+-----+\n",
    "\n",
    "    fill_values: {'Name': 'Unknown', 'Age': 0}\n",
    "\n",
    "    Sample Output:\n",
    "    ----------\n",
    "    Output DataFrame:\n",
    "    +---+--------+---+\n",
    "    | ID|  Name  |Age|\n",
    "    +---+--------+---+\n",
    "    |  1|  John  | 25|\n",
    "    |  2| Unknown| 30|\n",
    "    |  3|  Mike  |  0|\n",
    "    |  4| Unknown|  0|\n",
    "    +---+--------+---+\n",
    "    \"\"\"\n",
    "    filled_df = df.fillna(fill_values)\n",
    "    return filled_df\n",
    "\n",
    "def process_phone_numbers(customers_df, country_codes_path):\n",
    "    \"\"\"\n",
    "    Processes customer phone numbers by cleaning and formatting them, and appends the appropriate country code.\n",
    "    \n",
    "    Parameters:\n",
    "    - customers_df: Spark DataFrame containing customer data with columns including 'Phone' and 'Country'.\n",
    "    - country_codes_path: Path to the CSV file containing country codes with columns 'Country' and 'Country_Code'.\n",
    "\n",
    "    Returns:\n",
    "    - Spark DataFrame with cleaned phone numbers and appended country codes.\n",
    "\n",
    "     \n",
    "    Sample Input:\n",
    "    ----------\n",
    "    customers_df:\n",
    "    +----------+------------+----------------+\n",
    "    |CustomerID|   Country   |   Phone       |\n",
    "    +----------+------------+----------------+\n",
    "    |    1     |   USA       |(123) 456-7890 |\n",
    "    |    2     |   UK        | 123-456-7890  |\n",
    "    |    3     |   India     | +91-9876543210|\n",
    "    +----------+------------+----------------+\n",
    "\n",
    "    country_codes.csv:\n",
    "    +---------+--------------+\n",
    "    | Country | Country_Code |\n",
    "    +---------+--------------+\n",
    "    |  USA    |   +1         |\n",
    "    |  UK     |   +44        |\n",
    "    |  India  |   +91        |\n",
    "    +---------+--------------+\n",
    "\n",
    "    Sample Output:\n",
    "    ----------\n",
    "    +----------+------------+---------------+\n",
    "    |CustomerID|   Country  |   Phone       |\n",
    "    +----------+------------+---------------+\n",
    "    |    1     |   USA      |+1-1234567890  |\n",
    "    |    2     |   UK       |+44-1234567890 |\n",
    "    |    3     |   India    |+91-9876543210 |\n",
    "    +----------+------------+---------------+\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Load country codes data from CSV\n",
    "    country_codes_df = spark.read.csv(country_codes_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Step 2: Identify and remove the '+1-' prefix from phone numbers\n",
    "    cleaned_df = customers_df.withColumn(\n",
    "        \"Processed_Phone\",\n",
    "        when(col(\"Phone\").startswith(\"+1-\"), regexp_replace(col(\"Phone\"), r\"^\\+1-\", \"\"))\n",
    "        .otherwise(col(\"Phone\"))\n",
    "    )\n",
    "\n",
    "    # Step 3: Remove everything after 'x' or similar extensions\n",
    "    cleaned_df = cleaned_df.withColumn(\n",
    "        \"Processed_Phone\",\n",
    "        regexp_replace(col(\"Processed_Phone\"), r\"x.*\", \"\")\n",
    "    )\n",
    "\n",
    "    # Step 4: Remove non-digit characters to retain only numeric part of the phone number\n",
    "    final_cleaned_df = cleaned_df.withColumn(\n",
    "        \"Final_Phone_Number\",\n",
    "        regexp_replace(col(\"Processed_Phone\"), r\"[^0-9]\", \"\")\n",
    "    )\n",
    "\n",
    "    # Step 5: Join with the Country Codes DataFrame to add the Country Code\n",
    "    final_df = final_cleaned_df.join(country_codes_df, \"Country\", \"left\")\n",
    "\n",
    "    # Step 6: Append the country code to the cleaned phone number\n",
    "    final_df = final_df.withColumn(\n",
    "        \"Phone\",\n",
    "        when(col(\"Country_Code\").isNotNull() & col(\"Final_Phone_Number\").isNotNull(),\n",
    "             concat(col(\"Country_Code\"), lit(\"-\"), col(\"Final_Phone_Number\"))\n",
    "        ).otherwise(col(\"Phone\"))\n",
    "    )\n",
    "\n",
    "    # Select the final columns to include in the output DataFrame\n",
    "    final_df = final_df.select(\n",
    "        \"Customer_ID\", \n",
    "        \"Name\", \n",
    "        \"Email\", \n",
    "        \"Phone\", \n",
    "        \"Country\"\n",
    "    )\n",
    "\n",
    "    # Return the final DataFrame with the cleaned and formatted phone numbers\n",
    "    return final_df\n",
    "\n",
    "def validate_emails(customers_df):\n",
    "    \"\"\"\n",
    "    Validates the email addresses in the provided DataFrame. The function performs three main tasks:\n",
    "    1. Ensures all non-null emails are unique.\n",
    "    2. Extracts and lists unique email domains.\n",
    "    3. Validates the format of each email against a standard regex pattern.\n",
    "\n",
    "    Parameters:\n",
    "    customers_df (DataFrame): A DataFrame containing customer data, including an 'Email' column.\n",
    "\n",
    "    Returns:\n",
    "    None: Prints results to standard output.\n",
    "    \"\"\"\n",
    "    if 'Email' not in customers_df.columns:\n",
    "        raise ValueError(\"DataFrame must contain an 'Email' column\")\n",
    "\n",
    "    non_null_emails_df = customers_df[customers_df['Email'].notnull()]\n",
    "\n",
    "    # Check for unique emails\n",
    "    duplicate_emails = non_null_emails_df[non_null_emails_df.duplicated(subset='Email', keep=False)]\n",
    "    if duplicate_emails.empty:\n",
    "        print(\"All non-null emails are unique.\")\n",
    "    else:\n",
    "        print(\"There are duplicate emails in the dataset:\")\n",
    "        print(duplicate_emails[['Email']].drop_duplicates())\n",
    "\n",
    "    # Extract unique email domains\n",
    "    non_null_emails_df['Domain'] = non_null_emails_df['Email'].apply(lambda email: email.split('@')[-1])\n",
    "    unique_domains = non_null_emails_df['Domain'].drop_duplicates()\n",
    "    print(\"Unique email domains:\")\n",
    "    print(unique_domains)\n",
    "\n",
    "    # Validate email format\n",
    "    invalid_emails = []\n",
    "    for email in non_null_emails_df['Email']:\n",
    "        try:\n",
    "            validate_email(email)\n",
    "        except EmailNotValidError:\n",
    "            invalid_emails.append(email)\n",
    "\n",
    "    if not invalid_emails:\n",
    "        print(\"All non-null emails are valid.\")\n",
    "    else:\n",
    "        print(\"There are invalid emails in the dataset:\")\n",
    "        print(pd.DataFrame(invalid_emails, columns=['Email']))\n",
    "\n",
    "    return customers_df\n",
    "\n",
    "   \n",
    "\n",
    "def date_validation(df: DataFrame,  column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    This function identifies and corrects future dates in the 'Date' column of a DataFrame.\n",
    "    If future dates are found, they are replaced with the most recent valid past date in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing the 'Date' column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with future dates corrected.\n",
    "\n",
    "    Sample Input:\n",
    "    ----------\n",
    "    Input DataFrame:\n",
    "    +---+----------+\n",
    "    | ID|   Date   |\n",
    "    +---+----------+\n",
    "    |  1|2024-08-25|\n",
    "    |  2|2025-01-01|  # Future date\n",
    "    |  3|2023-12-15|\n",
    "    |  4|2024-07-10|\n",
    "    +---+----------+\n",
    "\n",
    "    Sample Output:\n",
    "    ----------\n",
    "    Output DataFrame:\n",
    "    +---+----------+\n",
    "    | ID|   Date   |\n",
    "    +---+----------+\n",
    "    |  1|2024-08-25|\n",
    "    |  2|2024-08-25|  # Corrected to most recent past date\n",
    "    |  3|2023-12-15|\n",
    "    |  4|2024-07-10|\n",
    "    +---+----------+\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Identify rows where 'Interaction_Date' is in the future\n",
    "    print(\"Step 1: Identifying future dates in 'Interaction_Date'...\")\n",
    "    future_dates_df = df.filter(col(column) > current_date())\n",
    "\n",
    "    # Check if there are any future dates\n",
    "    if future_dates_df.count() > 0:\n",
    "        print(f\"Found {future_dates_df.count()} records with future dates.\")\n",
    "        \n",
    "        # Step 2: Determine the most recent valid date in the dataset\n",
    "        most_recent_past_date = df.filter(col(column) <= current_date()) \\\n",
    "                                  .agg(spark_max(column)) \\\n",
    "                                  .collect()[0][0]\n",
    "\n",
    "        print(f\"The most recent valid past date is: {most_recent_past_date}\")\n",
    "\n",
    "        # Step 3: Replace future dates with the most recent valid past date\n",
    "        print(\"Step 3: Replacing future dates with the most recent valid past date...\")\n",
    "        df = df.withColumn(\n",
    "            column,\n",
    "            when(col(column) > current_date(), most_recent_past_date)\n",
    "            .otherwise(col(column))\n",
    "        )\n",
    "        \n",
    "        print(\"Future dates have been corrected.\")\n",
    "    else:\n",
    "        print(\"No future dates found.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def validate_boolean_values(df: DataFrame, column: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    This function validates the 'boolean' column to ensure it contains only boolean values.\n",
    "    Non-boolean values will be corrected to False by default.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing the 'Issue_Resolved' column.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The DataFrame with invalid 'Issue_Resolved' values corrected.\n",
    "\n",
    "    Sample Input:\n",
    "    ----------\n",
    "    Input DataFrame:\n",
    "    +---+--------------+\n",
    "    | ID|Issue_Resolved|\n",
    "    +---+--------------+\n",
    "    |  1|     True     |\n",
    "    |  2|     False    |\n",
    "    |  3|     Yes      |  # Non-boolean value\n",
    "    |  4|     1        |  # Non-boolean value\n",
    "    +---+--------------+\n",
    "\n",
    "    Sample Output:\n",
    "    ----------\n",
    "    Output DataFrame:\n",
    "    +---+--------------+\n",
    "    | ID|Issue_Resolved|\n",
    "    +---+--------------+\n",
    "    |  1|     True     |\n",
    "    |  2|     False    |\n",
    "    |  3|     False    |  # Corrected to False\n",
    "    |  4|     False    |  # Corrected to False\n",
    "    +---+--------------+\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Identify non-boolean values in 'Issue_Resolved'\n",
    "    print(\"Step 1: Identifying non-boolean values.. \")\n",
    "    \n",
    "    # Assuming the column should contain only True or False values\n",
    "    invalid_values_df = df.filter(~col(column).isin(True, False))\n",
    "\n",
    "    if invalid_values_df.count() > 0:\n",
    "        print(f\"Found {invalid_values_df.count()} records with non-boolean values in 'Issue_Resolved'.\")\n",
    "        \n",
    "        # Step 2: Correct non-boolean values to False\n",
    "        print(\"Step 2: Correcting non-boolean values to False...\")\n",
    "        df = df.withColumn(\n",
    "            column,\n",
    "            when(col(column).isin(True, False), col(column))\n",
    "            .otherwise(lit(False))\n",
    "        )\n",
    "        \n",
    "        print(\"Non-boolean values have been corrected to False.\")\n",
    "    else:\n",
    "        print(\"All values in 'Issue_Resolved' are valid booleans.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_df_to_csv(df, file_path):\n",
    "    \"\"\"\n",
    "    Saves a Spark DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Spark DataFrame to be saved.\n",
    "    - file_path: Path where the CSV file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Save DataFrame to CSV\n",
    "    df.toPandas().to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
