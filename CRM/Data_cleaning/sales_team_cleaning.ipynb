{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/14 13:56:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/14 13:56:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/14 13:56:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "%run /spark-data/CRM/utilities/common_utility.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/14 13:56:07 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = initialize_spark_session(\"Sales Team Cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:56:07,268 - logger - INFO - \u001b[92mLogger initialized with dynamic path!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "log_file_path = 'logs/sales_team_cleaning.log'\n",
    "logger = initialize_logger(log_file_path)\n",
    "\n",
    "logger.info(\"Logger initialized with dynamic path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:56:11,773 - logger - INFO - \u001b[92mDisplayed first 5 records of Spark DataFrame.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------------+-------------+------------+--------------+\n",
      "|Sales_Rep_ID                        |Name             |Region       |Sales_Target|Sales_Achieved|\n",
      "+------------------------------------+-----------------+-------------+------------+--------------+\n",
      "|0437b05a-9628-43f9-ac07-0b9a0dc96dcd|Brittany Taylor  |California   |41135       |14037.0       |\n",
      "|4daeb6af-d7e9-4f99-91b3-6c912f45b740|Mitchell Williams|New Hampshire|32996       |21461.0       |\n",
      "|f243144e-485f-4382-81ef-2a9a3c63f172|John Terry       |Kansas       |10385       |NULL          |\n",
      "|9c44ee81-8254-45e1-af23-a4608ceb126c|Carolyn Miller   |Arizona      |23754       |17149.0       |\n",
      "|3e97b5d8-933a-4860-bce7-2398af6c5613|Antonio Sparks   |Washington   |27101       |36413.0       |\n",
      "+------------------------------------+-----------------+-------------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_team_path = \"/spark-data/CRM/Dataset/sales_team.csv\"\n",
    "sales_team_df = load_data_files(sales_team_path)\n",
    "display_dataframes(sales_team_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:56:11,787 - logger - INFO - \u001b[92mStep 1: Identifying missing values in each column before filling them...\u001b[0m\n",
      "2024-09-14 13:56:12,617 - logger - INFO - \u001b[92mStep 2: Handling missing values by filling with averages...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+------------+--------------+\n",
      "|Sales_Rep_ID|Name|Region|Sales_Target|Sales_Achieved|\n",
      "+------------+----+------+------------+--------------+\n",
      "|           0|   0|     0|           0|             5|\n",
      "+------------+----+------+------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:56:12,940 - logger - INFO - \u001b[92mStep 3: Checking for duplicate records based on 'Sales_Rep_ID'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average value of sales achieved column is 21912.425531914894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:56:15,463 - logger - INFO - \u001b[92mStep 4: Dropping duplicate records based on 'Sales_Rep_ID'...\u001b[0m\n",
      "2024-09-14 13:56:15,859 - logger - INFO - \u001b[92mNumber of duplicate records before dropping: 2\u001b[0m\n",
      "2024-09-14 13:56:16,382 - logger - INFO - \u001b[92mNumber of duplicate records after dropping: 0\u001b[0m\n",
      "2024-09-14 13:56:16,385 - logger - INFO - \u001b[92mStep 5: Standardizing the format of 'Name' and 'Region' columns...\u001b[0m\n",
      "2024-09-14 13:56:16,432 - logger - INFO - \u001b[92mStep 6: Identifying missing values in each column after filling them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:56:17,208 - logger - INFO - \u001b[92mStep 7: Displaying the cleaned 'sales_team_df' DataFrame...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+------------+--------------+\n",
      "|Sales_Rep_ID|Name|Region|Sales_Target|Sales_Achieved|\n",
      "+------------+----+------+------------+--------------+\n",
      "|           0|   0|     0|           0|             0|\n",
      "+------------+----+------+------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:56:17,651 - logger - INFO - \u001b[92mStep 8: Saving the cleaned data to 'cleaned_sales_team.csv'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---------------------+------------+------------+--------------+\n",
      "|Sales_Rep_ID                        |Name                 |Region      |Sales_Target|Sales_Achieved|\n",
      "+------------------------------------+---------------------+------------+------------+--------------+\n",
      "|02acdbab-d149-4053-8c7a-871ba76f003e|Daniel Barber        |Arizona     |13651       |7296.0        |\n",
      "|0437b05a-9628-43f9-ac07-0b9a0dc96dcd|Brittany Taylor      |California  |41135       |14037.0       |\n",
      "|0468e32e-644b-491f-a3e1-741306f2c3f2|Dr. Wayne Spencer Dvm|Pennsylvania|15593       |37653.0       |\n",
      "|05cc8513-8a8a-4598-862a-218d1093ae26|Pamela Pennington    |Idaho       |38399       |11569.0       |\n",
      "|06a9a95f-ae20-44ae-ad5d-d4bdf5f121d0|Sean Snyder          |Illinois    |19165       |11748.0       |\n",
      "+------------------------------------+---------------------+------------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 13:56:17,956 - logger - INFO - \u001b[92mData cleaning and export completed successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after cleaning: 50\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify Missing Values\n",
    "logger.info(\"Step 1: Identifying missing values in each column before filling them...\")\n",
    "missing_values_before = count_missing_values(sales_team_df)\n",
    "missing_values_before.show()\n",
    "\n",
    "# Step 2: Handle Missing Values\n",
    "logger.info(\"Step 2: Handling missing values by filling with averages...\")\n",
    "# Calculate average Sales_Achieved\n",
    "avg_sales_achieved = sales_team_df.select(avg(col(\"Sales_Achieved\"))).first()[0]\n",
    "print(f\"Average value of sales achieved column is {avg_sales_achieved}\")\n",
    "\n",
    "# Fill missing Sales_Achieved with the average\n",
    "sales_team_cleaned_df = fill_missing_values(sales_team_df,{\"Sales_Achieved\": round(avg_sales_achieved,2)})\n",
    "\n",
    "# Step 3: Check for Duplicate Values\n",
    "logger.info(\"Step 3: Checking for duplicate records based on 'Sales_Rep_ID'...\")\n",
    "duplicate_count_before = count_duplicates_per_column(sales_team_cleaned_df)\n",
    "\n",
    "# Step 4: Drop Duplicates if they exist\n",
    "logger.info(\"Step 4: Dropping duplicate records based on 'Sales_Rep_ID'...\")\n",
    "sales_team_cleaned_df = drop_duplicates(sales_team_cleaned_df, \"Sales_Rep_ID\")\n",
    "\n",
    "# Step 6: Standardize Formats\n",
    "logger.info(\"Step 5: Standardizing the format of 'Name' and 'Region' columns...\")\n",
    "sales_team_cleaned_df = capitalize_columns(sales_team_cleaned_df, [ \"Name\", \"Region\"])\n",
    "print(\"Completed.\")\n",
    "\n",
    "# Step 7: Cross-verification of missing values in each column after filling them\n",
    "logger.info(\"Step 6: Identifying missing values in each column after filling them...\")\n",
    "missing_values_after = count_missing_values(sales_team_cleaned_df)\n",
    "missing_values_after.show()\n",
    "\n",
    "# Step 8: Display the cleaned 'sales_team_df' DataFrame\n",
    "logger.info(\"Step 7: Displaying the cleaned 'sales_team_df' DataFrame...\")\n",
    "sales_team_cleaned_df.show(5,truncate=False)\n",
    "\n",
    "# Step 9: Save the cleaned data to a new CSV\n",
    "logger.info(\"Step 8: Saving the cleaned data to 'cleaned_sales_team.csv'...\")\n",
    "# save_df_to_csv(sales_team_cleaned_df, \"Cleaned_data/cleaned_sales_team.csv\")\n",
    "\n",
    "# Display the count of records after phone number processing\n",
    "record_count_after_cleaning = sales_team_cleaned_df.count()\n",
    "print(f\"Number of records after cleaning: {record_count_after_cleaning}\")\n",
    "logger.info(\"Data cleaning and export completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
