{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/13 06:18:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "%run /spark-data/CRM/utilities/common_utility.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/13 06:18:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = initialize_spark_session(\"Interactions Cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:18:40,761 - logger - INFO - \u001b[92mLogger initialized with dynamic path!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "log_file_path = 'logs/interactions_cleaning.log'\n",
    "logger = initialize_logger(log_file_path)\n",
    "\n",
    "logger.info(\"Logger initialized with dynamic path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:18:43,729 - logger - INFO - \u001b[92mDisplayed first 5 records of Spark DataFrame.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "|Interaction_ID                      |Customer_ID                         |Interaction_Date|Interaction_Type|Issue_Resolved|\n",
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "|5a367006-c47a-4728-a042-c4c6ddd9ee3e|4166b61a-cb6d-4190-ac0b-df812c0308ff|2024-08-07      |NULL            |true          |\n",
      "|544d6348-cc9c-4c36-9fbe-eaf7e845e682|cc9a2115-b197-4a2b-9a6a-0fb2da5a0c73|2024-01-30      |Email           |true          |\n",
      "|b1739e10-0690-4ec6-9a6b-147127c0f388|1f22e566-13a0-4758-b42c-b0f6e997d46c|2024-06-27      |Chat            |true          |\n",
      "|3d9dcd53-7edd-4672-bfcf-a03b430b2935|dae0689d-0c38-440c-b921-fe2413c3df3b|2024-07-20      |Email           |false         |\n",
      "|2861521b-43ba-45d9-a0bd-9ee5c2e3edf4|c1e7a24a-0cc4-4a3e-8cf1-1e3487076ef4|2024-05-31      |Email           |false         |\n",
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interactions_file_path = \"/spark-data/CRM/Dataset/interactions.csv\"\n",
    "interactions_df = load_data_files(interactions_file_path)\n",
    "display_dataframes(interactions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:18:43,744 - logger - INFO - \u001b[92mStep 1: Identifying missing values in each column before filling them...\u001b[0m\n",
      "2024-09-13 06:18:44,369 - logger - INFO - \u001b[92mStep 2: Counting occurrences of each 'Interaction_Type' to find the most occurring type...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "|Interaction_ID|Customer_ID|Interaction_Date|Interaction_Type|Issue_Resolved|\n",
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "|             0|          0|               0|              67|             0|\n",
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:18:44,922 - logger - INFO - \u001b[92mStep 3: Replacing null values in 'Interaction_Type' with the most occurring type...\u001b[0m\n",
      "2024-09-13 06:18:44,950 - logger - INFO - \u001b[92mStep 4: Checking for duplicate records based on 'Interaction_ID'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most occurring 'Interaction_Type' is: Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:18:45,489 - logger - INFO - \u001b[92mNumber of duplicate records before dropping: 32\u001b[0m\n",
      "2024-09-13 06:18:45,912 - logger - INFO - \u001b[92mNumber of duplicate records after dropping: 0\u001b[0m\n",
      "2024-09-13 06:18:45,913 - logger - INFO - \u001b[92mStep 5: Capitalizing the first letter of each word in the and 'Interaction_Type' columns...\u001b[0m\n",
      "2024-09-13 06:18:45,925 - logger - INFO - \u001b[92mStep 6: Validating dates present in Interaction_Date column...\u001b[0m\n",
      "2024-09-13 06:18:45,925 - logger - INFO - \u001b[92mStep 1: Identifying future dates in 'Interaction_Date'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:18:46,306 - logger - INFO - \u001b[92mNo future dates found.\u001b[0m\n",
      "2024-09-13 06:18:46,309 - logger - INFO - \u001b[92mStep 7: Validating booleans present in Issue_Resolved column...\u001b[0m\n",
      "2024-09-13 06:18:46,310 - logger - INFO - \u001b[92mStep 1: Identifying non-boolean values.. \u001b[0m\n",
      "2024-09-13 06:18:46,700 - logger - INFO - \u001b[92mAll values in 'Issue_Resolved' are valid booleans.\u001b[0m\n",
      "2024-09-13 06:18:46,701 - logger - INFO - \u001b[92mStep 8: Identifying missing values in each column after filling them...\u001b[0m\n",
      "2024-09-13 06:18:47,194 - logger - INFO - \u001b[92mStep 9: Displaying the cleaned 'interactions_df' DataFrame...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "|Interaction_ID|Customer_ID|Interaction_Date|Interaction_Type|Issue_Resolved|\n",
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "|             0|          0|               0|               0|             0|\n",
      "+--------------+-----------+----------------+----------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:18:47,409 - logger - INFO - \u001b[92mStep 10: Saving the cleaned data to 'cleaned_interactions.csv'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "|Interaction_ID                      |Customer_ID                         |Interaction_Date|Interaction_Type|Issue_Resolved|\n",
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "|002debb8-7e9e-476c-85bc-dd3e8b7210ce|dfce9c24-71c4-4cd8-9b59-f34e48842979|2024-01-30      |Chat            |true          |\n",
      "|00571a5f-03ff-4615-85cf-cd540f2ccdad|bd56525d-e899-4312-9763-e79d3f6fa54c|2024-02-08      |Chat            |false         |\n",
      "|00960c9f-02c5-407c-8bb4-5d81747be2ea|f963092b-fa72-4017-bfa5-a7daec18bc46|2024-02-15      |Chat            |false         |\n",
      "|00f8180f-66fe-46a5-9d8c-caf7b97c23d1|0a1fae74-7af1-4032-8d72-0b0a14f9bd1b|2024-06-29      |Chat            |false         |\n",
      "|016b4c4f-4800-43d4-b6e8-f493fb01746d|b552b0ab-faf1-452b-a2e6-7b37a31aa881|2024-01-25      |Email           |true          |\n",
      "+------------------------------------+------------------------------------+----------------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:18:47,684 - logger - INFO - \u001b[92mData cleaning and export completed successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after cleaning: 800\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify missing values in each column before filling them\n",
    "logger.info(\"Step 1: Identifying missing values in each column before filling them...\")\n",
    "missing_values_before = count_missing_values(interactions_df)\n",
    "missing_values_before.show()\n",
    "\n",
    "# Step 2: Count occurrences of each 'Interaction_Type' and identify the most occurring type\n",
    "logger.info(\"Step 2: Counting occurrences of each 'Interaction_Type' to find the most occurring type...\")\n",
    "type_counts = interactions_df.groupBy(\"Interaction_Type\").count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .first()\n",
    "\n",
    "# Extract the most occurring 'Interaction_Type'\n",
    "most_occurring_type = type_counts[\"Interaction_Type\"] if type_counts else None\n",
    "print(f\"The most occurring 'Interaction_Type' is: {most_occurring_type}\")\n",
    "\n",
    "# Step 3: Replace only null values in 'Interaction_Type' with the most occurring type\n",
    "logger.info(\"Step 3: Replacing null values in 'Interaction_Type' with the most occurring type...\")\n",
    "cleaned_interactions_df = interactions_df.withColumn(\n",
    "    \"Interaction_Type\",\n",
    "    when(col(\"Interaction_Type\").isNull(), lit(most_occurring_type))\n",
    "    .otherwise(col(\"Interaction_Type\"))\n",
    ")\n",
    "\n",
    "# Step 4: Check for duplicate records based on 'Interaction_ID'\n",
    "logger.info(\"Step 4: Checking for duplicate records based on 'Interaction_ID'...\")\n",
    "cleaned_interactions_df = drop_duplicates(cleaned_interactions_df, \"Interaction_ID\")\n",
    "\n",
    "# Step 5: Capitalize the first letter of the values in the 'Issue_Resolved' and 'Interaction_Type' columns\n",
    "logger.info(\"Step 5: Capitalizing the first letter of each word in the and 'Interaction_Type' columns...\")\n",
    "cleaned_interactions_df = capitalize_columns(cleaned_interactions_df, [ \"Interaction_Type\"])\n",
    "print(\"Completed.\")\n",
    "\n",
    "# Step 6: date validation \n",
    "logger.info(\"Step 6: Validating dates present in Interaction_Date column...\")\n",
    "cleaned_interactions_df = date_validation(cleaned_interactions_df,\"Interaction_Date\")\n",
    "\n",
    "# Step 7: boolean validation  \n",
    "logger.info(\"Step 7: Validating booleans present in Issue_Resolved column...\")\n",
    "cleaned_interactions_df = validate_boolean_values(cleaned_interactions_df,\"Issue_Resolved\")\n",
    "\n",
    "# Step 8: Cross-verification of missing values in each column after filling them\n",
    "logger.info(\"Step 8: Identifying missing values in each column after filling them...\")\n",
    "missing_values_after = count_missing_values(cleaned_interactions_df)\n",
    "missing_values_after.show()\n",
    "\n",
    "# Step 9: Display the cleaned 'interactions_df' DataFrame\n",
    "logger.info(\"Step 9: Displaying the cleaned 'interactions_df' DataFrame...\")\n",
    "cleaned_interactions_df.show(5,truncate=False)\n",
    "\n",
    "# Step 10: Save the cleaned data to a new CSV\n",
    "logger.info(\"Step 10: Saving the cleaned data to 'cleaned_interactions.csv'...\")\n",
    "save_df_to_csv(cleaned_interactions_df, \"/spark-data/CRM/cleaned_data/cleaned_interactions.csv\")\n",
    "\n",
    "# Display the count of records after phone number processing\n",
    "record_count_after_cleaning = cleaned_interactions_df.count()\n",
    "print(f\"Number of records after cleaning: {record_count_after_cleaning}\")\n",
    "logger.info(\"Data cleaning and export completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
