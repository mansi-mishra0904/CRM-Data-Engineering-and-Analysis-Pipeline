{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/13 06:20:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "%run /spark-data/CRM/utilities/common_utility.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/13 06:20:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = initialize_spark_session(\"Products Cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:20:49,987 - logger - INFO - \u001b[92mLogger initialized with dynamic path!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "log_file_path = 'logs/products_cleaning.log'\n",
    "logger = initialize_logger(log_file_path)\n",
    "\n",
    "logger.info(\"Logger initialized with dynamic path!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:20:52,779 - logger - INFO - \u001b[92mDisplayed first 5 records of Spark DataFrame.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----------+------+\n",
      "|Product_ID|Product_Name  |Category   |Price |\n",
      "+----------+--------------+-----------+------+\n",
      "|1         |Sofa Set      |Home       |411.0 |\n",
      "|2         |Laptop        |Electronics|333.0 |\n",
      "|3         |Dining Table  |Home       |645.0 |\n",
      "|4         |Vacuum Cleaner|NULL       |290.0 |\n",
      "|5         |Mobile Phone  |Electronics|1738.0|\n",
      "+----------+--------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "products_file_path = \"/spark-data/CRM/Dataset/products.csv\"\n",
    "products_df = load_data_files(products_file_path)\n",
    "display_dataframes(products_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:20:52,794 - logger - INFO - \u001b[92mStep 1: Counting missing values in each column before filling them...\u001b[0m\n",
      "2024-09-13 06:20:53,353 - logger - INFO - \u001b[92mStep 2: Checking for duplicates in each column before dropping them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+-----+\n",
      "|Product_ID|Product_Name|Category|Price|\n",
      "+----------+------------+--------+-----+\n",
      "|         0|           0|       3|    0|\n",
      "+----------+------------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:20:55,968 - logger - INFO - \u001b[92mStep 3: Checking and removing duplicate records based on 'Customer_ID'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|      Column|Duplicate_Count|\n",
      "+------------+---------------+\n",
      "|  Product_ID|              1|\n",
      "|Product_Name|              1|\n",
      "|    Category|              5|\n",
      "|       Price|              2|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:20:56,214 - logger - INFO - \u001b[92mNumber of duplicate records before dropping: 1\u001b[0m\n",
      "2024-09-13 06:20:56,557 - logger - INFO - \u001b[92mNumber of duplicate records after dropping: 0\u001b[0m\n",
      "2024-09-13 06:20:56,558 - logger - INFO - \u001b[92mStep 4: Filling missing values in 'Email' and 'Phone' columns...\u001b[0m\n",
      "2024-09-13 06:20:56,942 - logger - INFO - \u001b[92mStep 5: Capitalizing the first letter of each word in the 'Product_Name' and 'Category' columns...\u001b[0m\n",
      "2024-09-13 06:20:56,973 - logger - INFO - \u001b[92mStep 6: Counting missing values in each column after filling them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+------+\n",
      "|Product_ID|Product_Name  |Category     |Price |\n",
      "+----------+--------------+-------------+------+\n",
      "|1         |Sofa Set      |Home         |411.0 |\n",
      "|2         |Laptop        |Electronics  |333.0 |\n",
      "|3         |Dining Table  |Home         |645.0 |\n",
      "|4         |Vacuum Cleaner|Uncategorized|290.0 |\n",
      "|5         |Mobile Phone  |Electronics  |1738.0|\n",
      "+----------+--------------+-------------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:20:57,346 - logger - INFO - \u001b[92mStep 7: Checking for duplicates in each column after dropping them...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+-----+\n",
      "|Product_ID|Product_Name|Category|Price|\n",
      "+----------+------------+--------+-----+\n",
      "|         0|           0|       0|    0|\n",
      "+----------+------------+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:20:59,088 - logger - INFO - \u001b[92mStep 8: Replacing negative or zero prices with the average price...\u001b[0m\n",
      "2024-09-13 06:20:59,245 - logger - INFO - \u001b[92mStep 9: Exporting the cleaned data to 'cleaned_products.csv'...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|      Column|Duplicate_Count|\n",
      "+------------+---------------+\n",
      "|  Product_ID|              0|\n",
      "|Product_Name|              0|\n",
      "|    Category|              5|\n",
      "|       Price|              1|\n",
      "+------------+---------------+\n",
      "\n",
      "completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 06:20:59,627 - logger - INFO - \u001b[92mData cleaning and export completed successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records after cleaning: 50\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Count missing (null) values for each column before filling them\n",
    "logger.info(\"Step 1: Counting missing values in each column before filling them...\")\n",
    "missing_values_before = count_missing_values(products_df)\n",
    "missing_values_before.show()\n",
    "\n",
    "# Step 2: Count duplicates before dropping them\n",
    "logger.info(\"Step 2: Checking for duplicates in each column before dropping them...\")\n",
    "duplicate_count = count_duplicates_per_column(products_df)\n",
    "duplicate_count.show()\n",
    "\n",
    "# Step 3: Drop duplicates based on 'Customer_ID' if any are found\n",
    "logger.info(\"Step 3: Checking and removing duplicate records based on 'Customer_ID'...\")\n",
    "cleaned_products_df = drop_duplicates(products_df, \"Product_ID\")\n",
    "\n",
    "# Step 4: Fill missing values in 'Email' and 'Phone' columns\n",
    "logger.info(\"Step 4: Filling missing values in 'Email' and 'Phone' columns...\")\n",
    "cleaned_products_df = fill_missing_values(cleaned_products_df, {\"Category\": \"Uncategorized\"})\n",
    "cleaned_products_df.show(5, truncate=False)\n",
    "\n",
    "# Step 5: Capitalize the first letter of the first and last names in the 'Name' and 'Country' columns\n",
    "logger.info(\"Step 5: Capitalizing the first letter of each word in the 'Product_Name' and 'Category' columns...\")\n",
    "cleaned_products_df = capitalize_columns(cleaned_products_df, [\"Product_Name\", \"Category\"])\n",
    "print(\"Completed.\")\n",
    "\n",
    "# Step 6: Cross-validation - Count missing values again after filling them\n",
    "logger.info(\"Step 6: Counting missing values in each column after filling them...\")\n",
    "missing_values_after = count_missing_values(cleaned_products_df)\n",
    "missing_values_after.show()\n",
    "\n",
    "# Step 7: Count duplicates after dropping them\n",
    "logger.info(\"Step 7: Checking for duplicates in each column after dropping them...\")\n",
    "duplicate_count_after = count_duplicates_per_column(cleaned_products_df)\n",
    "duplicate_count_after.show()\n",
    "\n",
    "# Step 8: Handle negative or zero prices by replacing with average price\n",
    "logger.info(\"Step 8: Replacing negative or zero prices with the average price...\")\n",
    "avg_price = products_df.agg({\"Price\": \"avg\"}).collect()[0][0]\n",
    "cleaned_products_df = cleaned_products_df.withColumn(\"Price\", when(col(\"Price\") <= 0, avg_price).otherwise(col(\"Price\")))\n",
    "print(\"completed.\")\n",
    "\n",
    "# Step 9: Export the cleaned data to a CSV file\n",
    "logger.info(\"Step 9: Exporting the cleaned data to 'cleaned_products.csv'...\")\n",
    "save_df_to_csv(cleaned_products_df, \"/spark-data/CRM/cleaned_data/cleaned_products.csv\")\n",
    "\n",
    "# show duplicates\n",
    "# get_duplicate_data_per_column(cleaned_products_df)\n",
    "\n",
    "# Display the count of records after phone number processing\n",
    "record_count_after_cleaning = cleaned_products_df.count()\n",
    "print(f\"Number of records after cleaning: {record_count_after_cleaning}\")\n",
    "\n",
    "logger.info(\"Data cleaning and export completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
